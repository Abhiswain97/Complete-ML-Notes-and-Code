{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b551473c-65c1-4cf8-9669-e753b9dd9706",
   "metadata": {},
   "source": [
    "---\n",
    "title: Linear Models\n",
    "author: Abhishek Swain\n",
    "date: 16 September, 2023\n",
    "format: \n",
    "    pdf: \n",
    "        documentclass: report\n",
    "        fontfamily: libertinus\n",
    "        colorlinks: true\n",
    "        toc: true\n",
    "        toc-depth: 3\n",
    "        toc-title: Table of Contents\n",
    "        keep-tex: true\n",
    "        lof: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98f0b6-f158-4266-b1c0-6739b8662f14",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b660ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import add_dummy_feature, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15baf485-7498-4e27-acf5-4e3d740b590e",
   "metadata": {},
   "source": [
    "## The Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c279b",
   "metadata": {},
   "source": [
    "### Standard equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97ba84-99b8-4320-b83a-48f6f1a9b9e1",
   "metadata": {},
   "source": [
    "$\\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\dots + \\theta_{n}x_{n}$ \n",
    "\n",
    "- $\\hat{y}$ is the predicted value.\n",
    "- n is the number of features.\n",
    "- $x_{i}$ is the $i^{th}$ feature value.\n",
    "- $\\theta_{j}$ is the $j^{th}$ model parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f429b7",
   "metadata": {},
   "source": [
    "### Vectorized equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e05e10",
   "metadata": {},
   "source": [
    "$\\hat{y} = h_{\\theta}(x) = \\theta \\cdot x$\n",
    "\n",
    "- $\\theta$ here is the model's parameter containing $\\theta_{0} \\dots \\theta_{n}$.\n",
    "- $x$ is the features containing $x_{0} \\dots x_{n}$ where $x_{0}$ is always 1.\n",
    "- $\\theta \\cdot x$ is the dot product between $\\theta$ & x. Both are colum vectors.\n",
    "\n",
    "\n",
    "$\\hat{y} = \\theta \\cdot x = \\theta^{T}x = [\\theta_{0} \\dots \\theta_{n}] \\begin{bmatrix} x_{0} \\\\ \\vdots \\\\ x_{n}\\end{bmatrix} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\dots + \\theta_{n}x_{n}$\n",
    "\n",
    "where $x_{0} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3ce12",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We need a loss function to train this model. Out loss funciton is MSE loss.\n",
    "\n",
    "$MSE(X, h_{\\theta}) = \\frac{1}{m} \\sum_{i=1}^m (\\theta^T x^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Here $(i)$ is the $i^{th}$ training example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa7c9e",
   "metadata": {},
   "source": [
    "### The Normal Equation\n",
    "\n",
    "Without using any optimization algorithm we also have a direct formula to get the parameters, this formula is the *Normal Equation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f940884",
   "metadata": {},
   "source": [
    "$\\hat{\\theta} = (X^{T}X)^{-1} X^{T} y$\n",
    "\n",
    "- $\\hat{\\theta}$ is the estimated parameter, $\\hat{\\theta} \\approx \\theta$\n",
    "- $y$ is is the target vector containing $y^{(1)} \\dots y^{(m)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8fd495",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0df40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.randn(m, 1)\n",
    "y = 3 * X + np.random.randn(m, 1)\n",
    "\n",
    "# Add bias term to X\n",
    "X_b = add_dummy_feature(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efffafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (100, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e811a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y # Using the normal eqaution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62bf9e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00742783],\n",
       "       [2.92837142]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a18ba8",
   "metadata": {},
   "source": [
    "The equation is $y = 3x + \\text{Gaussian Noise}$, and you can see that the estimated $\\theta_{1}$ is $2.9 \\approx 3$ and $\\theta_{0} = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0ba857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new = add_dummy_feature(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b2d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_new @ best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbcdfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00742783],\n",
       "       [5.86417067]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e74a1",
   "metadata": {},
   "source": [
    "### Linear Regression with `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2198357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_b, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde55729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 2.92837142]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e455d7",
   "metadata": {},
   "source": [
    "The `LinearRegression` in `sklearn` computes $X^{+}y$ where $X_{+}$ is the pseudoinverse (Moore - Penrose pseudoinverse). We can compute it directly using `np.linalg.pinv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ccafc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00742783],\n",
       "       [2.92837142]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e77567",
   "metadata": {},
   "source": [
    "### Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ebbf79",
   "metadata": {},
   "source": [
    "- Computing the inverse of $X^{T}X$ which is (n + 1) x (n + 1) matrix upto $O(n^{3})$. The scikit-learn Linear Regression is almost $O(n^{2})$\n",
    "- However, once computed, time to make predictions is very fast. It scales $O(n)$ with the number of predictions to be made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a83376",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "- This is an iterative method to find the parameters of the model\n",
    "- We compute the gradient of the loss function at a particular point and move in the direction of the -ve gradient (the steepest descent)\n",
    "\n",
    "Ex: You want to go down hill from a the top but it is very foggy, you use your feet to find the next steepest descent point\n",
    "\n",
    "We are basically searching in the model's parameter space, so more the parameters, the harder the search becomes.\n",
    "\n",
    "Factors affecting search:\n",
    "- Learning rate determines how big of a step we are taking, too big and we may overshoot\n",
    "- Shape of the function, if the loss function is complex (has lots of highs and toughs) there are chances our GD may get stuck in a *Local Minima* & not the *Global Minima*\n",
    "\n",
    "Fortunately, MSE is a convex function all we do is go down the slope to the bottom of the bowl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d76ea5",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f60789",
   "metadata": {},
   "source": [
    "To implement GD we need to compute the partial derivative of the Loss function with each parameter of the model\n",
    "\n",
    "$MSE(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (\\theta^T x^{(i)} - y^{(i)})^2$\n",
    "\n",
    "$\\frac{\\partial MSE(\\theta)}{\\partial \\theta_{j}} = \\frac{2}{m} \\sum_{i=1}^m (\\theta^T x^{(i)} - y^{(i)}) x_{j}$\n",
    "\n",
    "Rather than computing them one-by-one, we vectorize the whole stuff,\n",
    "\n",
    "$\\nabla_{\\theta}MSE(\\theta) = \\begin{bmatrix} \\frac{\\partial MSE(\\theta)}{\\partial \\theta_{0}} \\\\ \\frac{\\partial MSE(\\theta)}{\\partial \\theta_{1}} \\\\ \\vdots \\\\ \\frac{\\partial MSE(\\theta)}{\\partial \\theta_{n}} \\end{bmatrix} = \\frac{2}{m} X^{T} (X\\theta - y)$\n",
    "\n",
    "The gradient descent step:\n",
    "\n",
    "$\\theta = \\theta - \\eta\\nabla_{\\theta}MSE(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84fb7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick implementation\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "\n",
    "m = len(X_b)\n",
    "\n",
    "theta = np.random.randn(2, 1) # Sample theta from Gaussian distribution\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    grad = (2/m) * X_b.T @ (X_b @ theta -  y)\n",
    "    theta = theta - lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2356c0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00742783],\n",
       "       [2.92837142]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5faad1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00742783],\n",
       "       [2.92837142]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f46528",
   "metadata": {},
   "source": [
    "As we can see that the value we got from the normal equation and through GD is the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572a02d",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "- The issue with batch gradient descent is that it goes through the whole traininig set at once, making it very slow when the training set is huge. To solve this a better approach is SGD. It picks a random instance from the traning set and calculates the gradient based on that instance.\n",
    "- This causes the loss value to bounce up and down. So unlile BGD we will not see a gradual decrease but only seeing a decrease in loss on average.\n",
    "- This causes the loss to never settle even after reaching a minimum that also helps it get out of a local minima better than BGD. \n",
    "- To solve this we can use something called \"simulated annealing\", in which we keep decreasing the learning rate gradually (learning schedule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb6f825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "m =  len(X_b)\n",
    "\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        idx = np.random.randint(m)\n",
    "        Xi = X_b[idx:]\n",
    "        yi = y[idx:]\n",
    "        \n",
    "        grad = 2 * Xi.T @ (Xi @ theta - yi) # Do not divide by m, as it's a single instance\n",
    "        theta = theta - learning_schedule(epoch * m + i) * grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64b63a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01630277],\n",
       "       [2.92664477]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd738bc",
   "metadata": {},
   "source": [
    "We arrive at almost the same values but with just 50 epochs\n",
    "\n",
    "> All data need to be Identically and Independently distributed (IID) for SGD to work correctly, if the data is sorted by label then it will start optimizing for only one label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146513f",
   "metadata": {},
   "source": [
    "#### SGD with `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccbac60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(n_iter_no_change=100, random_state=42, tol=1e-05)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDRegressor(\n",
    "    max_iter=1000, \n",
    "    tol=1e-5, \n",
    "    eta0=0.01, \n",
    "    n_iter_no_change=100, \n",
    "    random_state=42\n",
    ")\n",
    "sgd.fit(X_b, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "909ae25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00404065, 2.92966112])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ba32d",
   "metadata": {},
   "source": [
    "#### Mini batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21b7b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [(X_b[i:i+10], y[i:i+10]) for i in range(0, len(X_b), 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab108288",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "theta = np.random.randn(2, 1)\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for xi, yi in batches:\n",
    "        m = len(xi)\n",
    "        grad = (2/m) * xi.T @ (xi @ theta - yi)\n",
    "        theta = theta - lr * grad   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1fdb7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00396903],\n",
       "       [2.92040931]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e507a",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f34cd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = 6 * np.random.randn(m, 1)\n",
    "y = 3 * X ** 2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6fe1524a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| label: Random-polynomial-data\n",
    "#| fig-cap: \"Random Polynomial data\"\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c5b406d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2fb7cfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.76348997]]), array([95.59508332]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04375a95",
   "metadata": {},
   "source": [
    "Simple LR doesn't do well, that is expected. We need to create some polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2aa7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = PolynomialFeatures(include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7cb45bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b6be839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.01748959, 2.99727606]]), array([2.17488155]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995cc1ef",
   "metadata": {},
   "source": [
    "This does so much better, our equation $y = 3X^{2} + X + 2 + \\text{Gaussian Noise}$, the predicted equation is $y = 2.99X^{2} + 1.017 X + 2.17$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc32960",
   "metadata": {},
   "source": [
    "# Bias/Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577c549",
   "metadata": {},
   "source": [
    "Model's generalization error is a sum of 3 different error:\n",
    "\n",
    "1. **Bias**: \n",
    "- Arising due to wrong assumptions like thinking data is linear when it is quadratic.\n",
    "- *High Bias* model mostly *Underfits*.\n",
    "\n",
    "2. **Variance**:\n",
    "- Arising due to model's excessive sensitivity to small variations in the data.\n",
    "- Model with high degrees of freedom likely has high variance.\n",
    "- *High Variance* model mostly *Overfits*.\n",
    "\n",
    "3. **Irreducible Error**\n",
    "- Arising due to noise in data.\n",
    "- Only way to solve is to clean the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74716603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225.35px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
