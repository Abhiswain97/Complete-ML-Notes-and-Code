{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b551473c-65c1-4cf8-9669-e753b9dd9706",
   "metadata": {},
   "source": [
    "---\n",
    "title: Linear Models\n",
    "author: Abhishek Swain\n",
    "date: 16 September, 2023\n",
    "format: \n",
    "    pdf: \n",
    "        documentclass: report\n",
    "        fontfamily: libertinus\n",
    "        colorlinks: true\n",
    "        toc: true\n",
    "        toc-title: Table of Contents\n",
    "        keep-tex: true\n",
    "        lof: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98f0b6-f158-4266-b1c0-6739b8662f14",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b660ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15baf485-7498-4e27-acf5-4e3d740b590e",
   "metadata": {},
   "source": [
    "## The equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c279b",
   "metadata": {},
   "source": [
    "### Standard equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97ba84-99b8-4320-b83a-48f6f1a9b9e1",
   "metadata": {},
   "source": [
    "$\\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\dots + \\theta_{n}x_{n}$ \n",
    "\n",
    "- $\\hat{y}$ is the predicted value.\n",
    "- n is the number of features.\n",
    "- $x_{i}$ is the $i^{th}$ feature value.\n",
    "- $\\theta_{j}$ is the $j^{th}$ model parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f429b7",
   "metadata": {},
   "source": [
    "### Vectorized equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e05e10",
   "metadata": {},
   "source": [
    "$\\hat{y} = h_{\\theta}(x) = \\theta \\cdot x$\n",
    "\n",
    "- $\\theta$ here is the model's parameter containing $\\theta_{0} \\dots \\theta_{n}$.\n",
    "- $x$ is the features containing $x_{0} \\dots x_{n}$ where $x_{0}$ is always 1.\n",
    "- $\\theta \\cdot x$ is the dot product between $\\theta$ & x. Both are colum vectors.\n",
    "\n",
    "\n",
    "$\\hat{y} = \\theta \\cdot x = \\theta^{T}x = [\\theta_{0} \\dots \\theta_{n}] \\begin{bmatrix} x_{0} \\\\ \\vdots \\\\ x_{n}\\end{bmatrix} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\dots + \\theta_{n}x_{n}$\n",
    "\n",
    "where $x_{0} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3ce12",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We need a loss function to train this model. Out loss funciton is MSE loss.\n",
    "\n",
    "$MSE(X, h_{\\theta}) = \\frac{1}{m} \\sum_{i=1}^m (\\theta^T x^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Here $(i)$ is the $i^{th}$ training example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa7c9e",
   "metadata": {},
   "source": [
    "### The Normal Equation\n",
    "\n",
    "Without using any optimization algorithm we also have a direct formula to get the parameters, this formula is the *Normal Equation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f940884",
   "metadata": {},
   "source": [
    "$\\hat{\\theta} = (X^{T}X)^{-1} X^{T} y$\n",
    "\n",
    "- $\\hat{\\theta}$ is the estimated parameter, $\\hat{\\theta} \\approx \\theta$\n",
    "- $y$ is is the target vector containing $y^{(1)} \\dots y^{(m)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8fd495",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb0df40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.randn(m, 1)\n",
    "y = 3 * X + np.random.randn(m, 1)\n",
    "\n",
    "# Add bias term to X\n",
    "X_b = add_dummy_feature(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4efffafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (100, 1))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e811a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y # Using the normal eqaution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62bf9e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00742783],\n",
       "       [2.92837142]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a18ba8",
   "metadata": {},
   "source": [
    "The equation is $y = 3x + \\text{Gaussian Noise}$, and you can see that the estimated $\\theta_{1}$ is $2.9 \\approx 3$ and $\\theta_{0} = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b0ba857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new = add_dummy_feature(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76b2d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_new @ best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fbcdfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00742783],\n",
       "       [5.86417067]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e74a1",
   "metadata": {},
   "source": [
    "### Linear Regression with `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2198357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_b, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bde55729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 2.92837142]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e455d7",
   "metadata": {},
   "source": [
    "The `LinearRegression` in `sklearn` computes $X^{+}y$ where $X_{+}$ is the pseudoinverse (Moore - Penrose pseudoinverse). We can compute it directly using `np.linalg.pinv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66ccafc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00742783],\n",
       "       [2.92837142]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e77567",
   "metadata": {},
   "source": [
    "### Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ebbf79",
   "metadata": {},
   "source": [
    "- Computing the inverse of $X^{T}X$ which is (n + 1) x (n + 1) matrix upto $O(n^{3})$. The scikit-learn Linear Regression is almost $O(n^{2})$\n",
    "- However, once computed, time to make predictions is very fast. It scales $O(n)$ with the number of predictions to be made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a83376",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "- This is an iterative method to find the parameters of the model\n",
    "- We compute the gradient of the loss function at a particular point and move in the direction of the -ve gradient (the steepest descent)\n",
    "\n",
    "Ex: You want to go down hill from a the top but it is very foggy, you use your feet to find the next steepest descent point\n",
    "\n",
    "We are basically searching in the model's parameter space, so more the parameters, the harder the search becomes.\n",
    "\n",
    "Factors affecting search:\n",
    "- Learning rate determines how big of a step we are taking, too big and we may overshoot\n",
    "- Shape of the function, if the loss function is complex (has lots of highs and toughs) there are chances our GD may get stuck in a *Local Minima* & not the *Global Minima*\n",
    "\n",
    "Fortunately, MSE is a convex function all we do is go down the slope to the bottom of the bowl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d76ea5",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f60789",
   "metadata": {},
   "source": [
    "To implement GD we need to compute the partial derivative of the Loss function with each parameter of the model\n",
    "\n",
    "$MSE(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (\\theta^T x^{(i)} - y^{(i)})^2$\n",
    "\n",
    "$\\frac{\\partial MSE(\\theta)}{\\partial \\theta_{j}} = \\frac{2}{m} \\sum_{i=1}^m (\\theta^T x^{(i)} - y^{(i)}) x_{j}$\n",
    "\n",
    "Rather than computing them one-by-one, we vectorize the whole stuff,\n",
    "\n",
    "$\\nabla_{\\theta}MSE(\\theta) = \\begin{bmatrix} \\frac{\\partial MSE(\\theta)}{\\partial \\theta_{0}} \\\\ \\frac{\\partial MSE(\\theta)}{\\partial \\theta_{1}} \\\\ \\vdots \\\\ \\frac{\\partial MSE(\\theta)}{\\partial \\theta_{n}} \\end{bmatrix} = \\frac{2}{m} X^{T} (X\\theta - y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10024d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225.35px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
