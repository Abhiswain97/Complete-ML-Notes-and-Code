% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{report}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Linear Models},
  pdfauthor={Abhishek Swain},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Linear Models}
\author{Abhishek Swain}
\date{2023-09-16}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[breakable, enhanced, interior hidden, frame hidden, sharp corners, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of Contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\hypertarget{linear-regression}{%
\chapter{Linear Regression}\label{linear-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ add\_dummy\_feature}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-equations}{%
\section{The equations}\label{the-equations}}

\hypertarget{standard-equation}{%
\subsection{Standard equation}\label{standard-equation}}

\(\hat{y} = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \dots + \theta_{n}x_{n}\)

\begin{itemize}
\tightlist
\item
  \(\hat{y}\) is the predicted value.
\item
  n is the number of features.
\item
  \(x_{i}\) is the \(i^{th}\) feature value.
\item
  \(\theta_{j}\) is the \(j^{th}\) model parameter.
\end{itemize}

\hypertarget{vectorized-equation}{%
\subsection{Vectorized equation}\label{vectorized-equation}}

\(\hat{y} = h_{\theta}(x) = \theta \cdot x\)

\begin{itemize}
\tightlist
\item
  \(\theta\) here is the model's parameter containing
  \(\theta_{0} \dots \theta_{n}\).
\item
  \(x\) is the features containing \(x_{0} \dots x_{n}\) where \(x_{0}\)
  is always 1.
\item
  \(\theta \cdot x\) is the dot product between \(\theta\) \& x. Both
  are colum vectors.
\end{itemize}

\(\hat{y} = \theta \cdot x = \theta^{T}x = [\theta_{0} \dots \theta_{n}] \begin{bmatrix} x_{0} \\ \vdots \\ x_{n}\end{bmatrix} = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \dots + \theta_{n}x_{n}\)

where \(x_{0} = 1\)

\hypertarget{training-the-model}{%
\section{Training the model}\label{training-the-model}}

We need a loss function to train this model. Out loss funciton is MSE
loss.

\(MSE(X, h_{\theta}) = \frac{1}{m} \sum_{i=1}^m (\theta^T x^{(i)} - y^{(i)})^2\)

Here \((i)\) is the \(i^{th}\) training example.

\hypertarget{the-normal-equation}{%
\subsection{The Normal Equation}\label{the-normal-equation}}

Without using any optimization algorithm we also have a direct formula
to get the parameters, this formula is the \emph{Normal Equation}

\(\hat{\theta} = (X^{T}X)^{-1} X^{T} y\)

\begin{itemize}
\tightlist
\item
  \(\hat{\theta}\) is the estimated parameter,
  \(\hat{\theta} \approx \theta\)
\item
  \(y\) is is the target vector containing \(y^{(1)} \dots y^{(m)}\)
\end{itemize}

\hypertarget{examples}{%
\subsubsection{Examples}\label{examples}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate dummy data}

\NormalTok{np.random.seed(}\DecValTok{42}\NormalTok{)}
\NormalTok{m }\OperatorTok{=} \DecValTok{100}
\NormalTok{X }\OperatorTok{=} \DecValTok{2} \OperatorTok{*}\NormalTok{ np.random.randn(m, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=} \DecValTok{3} \OperatorTok{*}\NormalTok{ X }\OperatorTok{+}\NormalTok{ np.random.randn(m, }\DecValTok{1}\NormalTok{)}

\CommentTok{\# Add bias term to X}
\NormalTok{X\_b }\OperatorTok{=}\NormalTok{ add\_dummy\_feature(X)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_b.shape, y.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
((100, 2), (100, 1))
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_theta }\OperatorTok{=}\NormalTok{ np.linalg.inv(X\_b.T }\OperatorTok{@}\NormalTok{ X\_b) }\OperatorTok{@}\NormalTok{ X\_b.T }\OperatorTok{@}\NormalTok{ y }\CommentTok{\# Using the normal eqaution}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_theta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[0.00742783],
       [2.92837142]])
\end{verbatim}

The equation is \(y = 3x + \text{Gaussian Noise}\), and you can see that
the estimated \(\theta_{1}\) is \(2.9 \approx 3\) and \(\theta_{0} = 0\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make some predictions}

\NormalTok{X\_new }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{], [}\DecValTok{2}\NormalTok{]])}
\NormalTok{X\_new }\OperatorTok{=}\NormalTok{ add\_dummy\_feature(X\_new)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ X\_new }\OperatorTok{@}\NormalTok{ best\_theta}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_pred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[0.00742783],
       [5.86417067]])
\end{verbatim}

\hypertarget{linear-regression-with-scikit-learn}{%
\subsection{\texorpdfstring{Linear Regression with
\texttt{scikit-learn}}{Linear Regression with scikit-learn}}\label{linear-regression-with-scikit-learn}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lr }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{lr.fit(X\_b, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LinearRegression()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lr.coef\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[0.        , 2.92837142]])
\end{verbatim}

The \texttt{LinearRegression} in \texttt{sklearn} computes \(X^{+}y\)
where \(X_{+}\) is the pseudoinverse (Moore - Penrose pseudoinverse). We
can compute it directly using \texttt{np.linalg.pinv}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.linalg.pinv(X\_b) }\OperatorTok{@}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[0.00742783],
       [2.92837142]])
\end{verbatim}

\hypertarget{computational-complexity}{%
\subsection{Computational Complexity}\label{computational-complexity}}

\begin{itemize}
\tightlist
\item
  Computing the inverse of \(X^{T}X\) which is (n + 1) x (n + 1) matrix
  upto \(O(n^{3})\). The scikit-learn Linear Regression is almost
  \(O(n^{2})\)
\item
  However, once computed, time to make predictions is very fast. It
  scales \(O(n)\) with the number of predictions to be made
\end{itemize}

\hypertarget{gradient-descent}{%
\subsection{Gradient Descent}\label{gradient-descent}}

\begin{itemize}
\tightlist
\item
  This is an iterative method to find the parameters of the model
\item
  We compute the gradient of the loss function at a particular point and
  move in the direction of the -ve gradient (the steepest descent)
\end{itemize}

Ex: You want to go down hill from a the top but it is very foggy, you
use your feet to find the next steepest descent point

We are basically searching in the model's parameter space, so more the
parameters, the harder the search becomes.

Factors affecting search: - Learning rate determines how big of a step
we are taking, too big and we may overshoot - Shape of the function, if
the loss function is complex (has lots of highs and toughs) there are
chances our GD may get stuck in a \emph{Local Minima} \& not the
\emph{Global Minima}

Fortunately, MSE is a convex function all we do is go down the slope to
the bottom of the bowl

\hypertarget{batch-gradient-descent}{%
\subsubsection{Batch Gradient Descent}\label{batch-gradient-descent}}

To implement GD we need to compute the partial derivative of the Loss
function with each parameter of the model

\(MSE(\theta) = \frac{1}{m} \sum_{i=1}^m (\theta^T x^{(i)} - y^{(i)})^2\)

\(\frac{\partial MSE(\theta)}{\partial \theta_{j}} = \frac{2}{m} \sum_{i=1}^m (\theta^T x^{(i)} - y^{(i)}) x_{j}\)

Rather than computing them one-by-one, we vectorize the whole stuff,

\(\nabla_{\theta}MSE(\theta) = \begin{bmatrix} \frac{\partial MSE(\theta)}{\partial \theta_{0}} \\ \frac{\partial MSE(\theta)}{\partial \theta_{1}} \\ \vdots \\ \frac{\partial MSE(\theta)}{\partial \theta_{n}} \end{bmatrix} = \frac{2}{m} X^{T} (X\theta - y)\)



\end{document}
